{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os  # For interacting with the operating system (e.g., file paths)\n",
    "import pandas as pd  # For working with data in table format\n",
    "import numpy as np  # For numerical operations\n",
    "from transformers import DistilBertTokenizer, DistilBertModel  # For processing and embedding text\n",
    "import torch  # For machine learning operations\n",
    "import PyPDF2  # For reading PDF files\n",
    "import logging  # For logging messages (errors, info, etc.)\n",
    "from tqdm import tqdm  # For showing progress bars\n",
    "import warnings  # For handling warning messages\n",
    "\n",
    "# Set up logging to keep track of what's happening in the program\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ignore some specific warnings that aren't important for our task\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas.core.arrays.masked\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"PyPDF2\")\n",
    "\n",
    "# Initialize the DistilBERT model and tokenizer\n",
    "# This is a pre-trained AI model that helps us understand and process text\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Set up the device (GPU if available, otherwise CPU) for faster processing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the chosen device\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read content from CSV, PDF, or TXT file.\n",
    "    This function tries to read different file types and return their content.\n",
    "    \"\"\"\n",
    "    # Get the file extension (e.g., .csv, .pdf, .txt)\n",
    "    file_type = os.path.splitext(file_path)[1].lower()\n",
    "    try:\n",
    "        if file_type == '.csv':\n",
    "            # If it's a CSV file, read it and join all the text\n",
    "            df = pd.read_csv(file_path)\n",
    "            return ' '.join(df.astype(str).values.flatten())\n",
    "        elif file_type == '.pdf':\n",
    "            # If it's a PDF, use the read_pdf function (defined below)\n",
    "            return read_pdf(file_path)\n",
    "        elif file_type == '.txt':\n",
    "            # If it's a text file, simply read its contents\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        else:\n",
    "            # If it's not a supported file type, log a warning and return None\n",
    "            logger.warning(f\"Unsupported file type: {file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # If there's any error in reading the file, log it and return None\n",
    "        logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Read content from PDF file with error handling.\n",
    "    This function tries to extract text from each page of a PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    # Try to extract text from each page\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    # If there's an error with a specific page, log it and continue\n",
    "                    logger.warning(f\"Error extracting text from page in {file_path}: {str(e)}\")\n",
    "            return text if text.strip() else None\n",
    "    except PyPDF2.errors.PdfReadError as e:\n",
    "        # If there's an error reading the PDF, log it and return None\n",
    "        logger.error(f\"PyPDF2 error reading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # If there's any other unexpected error, log it and return None\n",
    "        logger.error(f\"Unexpected error reading PDF {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Split text into chunks.\n",
    "    This function breaks long text into smaller pieces for easier processing.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate embedding for a single text using DistilBERT.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def batch_get_embeddings(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of texts.\n",
    "    This processes multiple texts at once for efficiency.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\", unit=\"batch\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = [get_embedding(text) for text in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def process_documents(input_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Process documents and create vector database.\n",
    "    This function reads all documents in a directory, processes them, and saves the results.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    processed_files = 0\n",
    "    skipped_files = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    # Walk through all files in the input directory\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in tqdm(files, desc=\"Processing files\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            document_id = os.path.splitext(file)[0]\n",
    "            text = read_file(file_path)\n",
    "            if text:\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk in chunks:\n",
    "                    all_chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'document_id': document_id,\n",
    "                        'document_file': file_path,\n",
    "                        'chunk_text': chunk\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                processed_files += 1\n",
    "            else:\n",
    "                skipped_files += 1\n",
    "\n",
    "    if not all_chunks:\n",
    "        logger.info(\"No documents processed successfully.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Generating embeddings for {len(all_chunks)} chunks\")\n",
    "    df = pd.DataFrame(all_chunks)\n",
    "    df['vector_embedding'] = batch_get_embeddings(df['chunk_text'].tolist())\n",
    "\n",
    "    logger.info(f\"Saving to {output_csv}\")\n",
    "    # Convert embeddings to string for CSV storage\n",
    "    df['vector_embedding'] = df['vector_embedding'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    logger.info(f\"Processed {processed_files} files successfully, skipped {skipped_files} files, created {len(df)} chunks\")\n",
    "\n",
    "def search_similar(query, df, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    df['vector_embedding'] = df['vector_embedding'].apply(lambda x: np.fromstring(x, sep=','))\n",
    "    df['similarity'] = df['vector_embedding'].apply(lambda x: np.dot(x, query_embedding))\n",
    "    return df.sort_values('similarity', ascending=False).head(top_k)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"documents\"\n",
    "    output_csv = \"vector_final_db.csv\"\n",
    "    process_documents(input_dir, output_csv)\n",
    "    # df = pd.read_csv(output_csv)\n",
    "    # query = \"deep learning in computer vision\"\n",
    "    # results = search_similar(query, df)\n",
    "    # print(results[['chunk_text', 'document_file', 'similarity']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
